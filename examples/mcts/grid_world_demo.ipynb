{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.tree_util as jtu\n",
    "from jax import nn, vmap, random, lax\n",
    "from typing import List, Optional\n",
    "from jaxtyping import Array\n",
    "from jax import random as jr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pymdp.envs import GridWorldEnv\n",
    "from pymdp.jax.control import construct_policies\n",
    "from pymdp.jax.agent import Agent as AIFAgent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid world generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 23:14:09.315056: W external/xla/xla/service/gpu/nvptx_compiler.cc:763] The NVIDIA driver's CUDA version is 12.4 which is older than the ptxas CUDA version (12.5.40). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "num_rows, num_columns = 7, 7\n",
    "num_states = [num_rows*num_columns] # number of states equals the number of grid locations\n",
    "num_obs = [num_rows*num_columns]    # number of observations equals the number of grid locations (fully observable)\n",
    "\n",
    "# number of agents\n",
    "n_batches = 3\n",
    "\n",
    "# construct A arrays\n",
    "A = [jnp.broadcast_to(jnp.eye(num_states[0]), (n_batches,) + (num_obs[0], num_states[0]))] # fully observable (identity observation matrix\n",
    "\n",
    "# construct B arrays\n",
    "grid_world = GridWorldEnv(shape=[num_rows, num_columns])\n",
    "B = [jnp.broadcast_to(jnp.array(grid_world.get_transition_dist()), (n_batches,) + (num_states[0], num_states[0], grid_world.n_control))]  # easy way to get the generative model parameters is to extract them from one of pre-made GridWorldEnv classes\n",
    "num_controls = [grid_world.n_control] # number of control states equals the number of actions\n",
    " \n",
    "# create mapping from gridworld coordinates to linearly-index states\n",
    "grid = np.arange(grid_world.n_states).reshape(grid_world.shape)\n",
    "it = np.nditer(grid, flags=[\"multi_index\"])\n",
    "coord_to_idx_map = {}\n",
    "while not it.finished:\n",
    "    coord_to_idx_map[it.multi_index] = it.iterindex\n",
    "    it.iternext()\n",
    "\n",
    "# construct C arrays\n",
    "desired_position = (6, 6) # lower corner\n",
    "desired_state_id = coord_to_idx_map[desired_position]\n",
    "desired_obs_id = jnp.argmax(A[0][:, desired_state_id]) # throw this in there, in case there is some indeterminism between states and observations\n",
    "C = [jnp.broadcast_to(nn.one_hot(desired_obs_id, num_obs[0]), (n_batches, num_obs[0]))]\n",
    "\n",
    "# construct D arrays\n",
    "starting_position = (3, 3) # middle\n",
    "# starting_position = (0, 0) # upper left corner\n",
    "starting_state_id = coord_to_idx_map[starting_position]\n",
    "starting_obs_id = jnp.argmax(A[0][:, starting_state_id]) # throw this in there, in case there is some indeterminism between states and observations\n",
    "D = [jnp.broadcast_to(nn.one_hot(starting_state_id, num_states[0]), (n_batches, num_states[0]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "planning_horizon = 1\n",
    "max_depth = 12\n",
    "policy_matrix = construct_policies(num_states, num_controls, policy_len=planning_horizon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize an `Agent()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create agent\n",
    "agent = AIFAgent(\n",
    "    A,\n",
    "    B,\n",
    "    C,\n",
    "    D,\n",
    "    E=None,\n",
    "    pA=None,\n",
    "    pB=None,\n",
    "    policies=policy_matrix,\n",
    "    policy_len=planning_horizon,\n",
    "    use_utility=True,\n",
    "    use_states_info_gain=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCTS based policy search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mctx\n",
    "from tmp_mcts import make_aif_recurrent_fn\n",
    "\n",
    "def si_policy(rng_key, agent, beliefs):\n",
    "    root = mctx.RootFnOutput(\n",
    "        prior_logits=jnp.log(agent.E),\n",
    "        value=jnp.zeros((agent.batch_size)),\n",
    "        embedding=beliefs,\n",
    "    )\n",
    "\n",
    "    recurrent_fn = make_aif_recurrent_fn()\n",
    "\n",
    "    policy_output = mctx.gumbel_muzero_policy(\n",
    "        agent,\n",
    "        rng_key,\n",
    "        root,\n",
    "        recurrent_fn,\n",
    "        num_simulations=4096,\n",
    "        max_depth=max_depth\n",
    "    )\n",
    "\n",
    "    return policy_output.action_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run active inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid position for agent 2 at time 0: (3, 3)\n",
      "[[0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 1.9621882e-31 1.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00]]\n",
      "Grid position for agent 2 at time 1: (4, 3)\n",
      "[[0.2 0.2 0.2 0.2 0.2]\n",
      " [0.  0.  0.  0.  1. ]\n",
      " [0.  0.  1.  0.  0. ]]\n",
      "Grid position for agent 2 at time 2: (4, 3)\n",
      "[[0.        0.        1.        0.        0.       ]\n",
      " [0.        1.        0.        0.        0.       ]\n",
      " [0.        0.4717898 0.5282102 0.        0.       ]]\n",
      "Grid position for agent 2 at time 3: (4, 4)\n",
      "[[2.0000000e-01 2.0000000e-01 2.0000000e-01 2.0000000e-01 2.0000000e-01]\n",
      " [0.0000000e+00 3.6696893e-01 6.3303107e-01 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 1.0000000e+00 4.7584586e-10 0.0000000e+00 3.0332685e-25]]\n",
      "Grid position for agent 2 at time 4: (5, 4)\n",
      "[[2.0000000e-01 2.0000000e-01 2.0000000e-01 2.0000000e-01 2.0000000e-01]\n",
      " [0.0000000e+00 4.7349367e-01 5.2650636e-01 0.0000000e+00 9.0701742e-26]\n",
      " [0.0000000e+00 1.0000000e+00 3.1228996e-18 0.0000000e+00 2.0406330e-31]]\n",
      "Grid position for agent 2 at time 5: (6, 4)\n",
      "[[2.0000000e-01 2.0000000e-01 2.0000000e-01 2.0000000e-01 2.0000000e-01]\n",
      " [0.0000000e+00 1.0000000e+00 3.1228996e-18 0.0000000e+00 2.0406330e-31]\n",
      " [0.0000000e+00 1.0000000e+00 2.3169819e-26 0.0000000e+00 3.8430610e-41]]\n",
      "Grid position for agent 2 at time 6: (6, 5)\n",
      "[[2.0000000e-01 2.0000000e-01 2.0000000e-01 2.0000000e-01 2.0000000e-01]\n",
      " [0.0000000e+00 1.0000000e+00 3.8430610e-41 0.0000000e+00 2.3169819e-26]\n",
      " [0.0000000e+00 1.0309456e-27 5.0186157e-01 0.0000000e+00 4.9813843e-01]]\n",
      "Grid position for agent 2 at time 7: (6, 6)\n",
      "[[2.0000000e-01 2.0000000e-01 2.0000000e-01 2.0000000e-01 2.0000000e-01]\n",
      " [0.0000000e+00 4.9813843e-01 1.0309456e-27 0.0000000e+00 5.0186157e-01]\n",
      " [0.0000000e+00 5.0186157e-01 1.0309456e-27 0.0000000e+00 4.9813843e-01]]\n",
      "Grid position for agent 2 at time 8: (6, 6)\n",
      "[[2.0000000e-01 2.0000000e-01 2.0000000e-01 2.0000000e-01 2.0000000e-01]\n",
      " [0.0000000e+00 1.0309456e-27 4.9813843e-01 0.0000000e+00 5.0186157e-01]\n",
      " [0.0000000e+00 5.0186157e-01 4.9813843e-01 0.0000000e+00 1.0309456e-27]]\n",
      "Grid position for agent 2 at time 9: (6, 6)\n",
      "[[2.0000000e-01 2.0000000e-01 2.0000000e-01 2.0000000e-01 2.0000000e-01]\n",
      " [0.0000000e+00 5.0186157e-01 1.0309456e-27 0.0000000e+00 4.9813843e-01]\n",
      " [0.0000000e+00 5.0186157e-01 4.9813843e-01 0.0000000e+00 1.0309456e-27]]\n",
      "Grid position for agent 2 at time 10: (6, 6)\n",
      "[[2.0000000e-01 2.0000000e-01 2.0000000e-01 2.0000000e-01 2.0000000e-01]\n",
      " [0.0000000e+00 1.0309456e-27 5.0186157e-01 0.0000000e+00 4.9813843e-01]\n",
      " [0.0000000e+00 1.0309456e-27 5.0186157e-01 0.0000000e+00 4.9813843e-01]]\n",
      "Grid position for agent 2 at time 11: (6, 6)\n",
      "[[2.0000000e-01 2.0000000e-01 2.0000000e-01 2.0000000e-01 2.0000000e-01]\n",
      " [0.0000000e+00 4.9813843e-01 5.0186157e-01 0.0000000e+00 1.0309456e-27]\n",
      " [0.0000000e+00 5.0186157e-01 1.0309456e-27 0.0000000e+00 4.9813843e-01]]\n",
      "Grid position for agent 2 at time 12: (6, 6)\n",
      "[[2.0000000e-01 2.0000000e-01 2.0000000e-01 2.0000000e-01 2.0000000e-01]\n",
      " [0.0000000e+00 1.0309456e-27 5.0186157e-01 0.0000000e+00 4.9813843e-01]\n",
      " [0.0000000e+00 5.0186157e-01 4.9813843e-01 0.0000000e+00 1.0309456e-27]]\n",
      "Grid position for agent 2 at time 13: (6, 6)\n",
      "[[2.0000000e-01 2.0000000e-01 2.0000000e-01 2.0000000e-01 2.0000000e-01]\n",
      " [0.0000000e+00 1.0309456e-27 4.9813843e-01 0.0000000e+00 5.0186157e-01]\n",
      " [0.0000000e+00 5.0186157e-01 4.9813843e-01 0.0000000e+00 1.0309456e-27]]\n"
     ]
    }
   ],
   "source": [
    "T = 8 # needed if you start further away from the goal (e.g. in upper left corner)\n",
    "\n",
    "obs_idx = [jnp.broadcast_to(starting_obs_id, (n_batches, 1))] # list of len (num_modalities), each list element of shape (n_batches, 1)\n",
    "\n",
    "state = jnp.broadcast_to(starting_state_id, (n_batches,))\n",
    "\n",
    "prior = agent.D\n",
    "key = jr.PRNGKey(101)\n",
    "batch_to_track = 1\n",
    "actions=None\n",
    "for t in range(T):\n",
    "\n",
    "    print('Grid position for agent {} at time {}: {}'.format(batch_to_track+1, t, np.unravel_index(state[batch_to_track], grid_world.shape)))\n",
    "\n",
    "    beliefs = agent.infer_states(obs_idx, prior)\n",
    "    embedings = jtu.tree_map(lambda x: x.squeeze(1), beliefs)\n",
    "    key, _key = jr.split(key)\n",
    "    q_pi = si_policy(_key, agent, embedings)\n",
    "    print(q_pi)\n",
    "\n",
    "    keys = jr.split(key, n_batches + 1)\n",
    "    batch_keys, key = keys[:-1], keys[-1]\n",
    "    actions = agent.sample_action(q_pi, rng_key=batch_keys)\n",
    "    (prior, _) = agent.update_empirical_prior(actions, beliefs)\n",
    "\n",
    "    # get next state and observation from the grid world (need to vmap everything over batches)\n",
    "    state = vmap(lambda b, s, a: jnp.argmax(b[:, s, a]), in_axes=(0,0,0))(B[0], state, actions)\n",
    "    next_obs = vmap(lambda a, s: jnp.argmax(a[:, s]), in_axes=(0,0))(A[0], state)\n",
    "    obs_idx = [next_obs]\n",
    "    obs_idx = jtu.tree_map(lambda x: jnp.expand_dims(x, -1), obs_idx) # add a trivial time dimension to the observation to enable indexing during agent.infer_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atari_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
