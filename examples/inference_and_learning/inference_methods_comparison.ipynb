{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import tree_util as jtu\n",
    "from pymdp.jax.agent import Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up generative model and a sequence of observations. The A tensors, B tensors and observations are specified in such a way that  only later observations ($o_{t > 1}$) help disambiguate hidden states at earlier time points. This will demonstrate the importance of \"smoothing\" or retrospective inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = [3, 2]\n",
    "num_obs = [3]\n",
    "\n",
    "A_tensor = jnp.stack([jnp.array([[0.5, 0.5, 0.], \n",
    "                                [0.0,  0.0,  1.], \n",
    "                                [0.5, 0.5, 0.]]\n",
    "                            ), jnp.array([[1./3, 1./3, 1./3], \n",
    "                                            [1./3, 1./3, 1./3], \n",
    "                                            [1./3, 1./3, 1./3]]\n",
    "                            )], axis=-1)\n",
    "\n",
    "A = [ jnp.broadcast_to(A_tensor, (2, 3, 3, 2)) ]\n",
    "\n",
    "# create two B matrices, one for each action\n",
    "B_1 = jnp.broadcast_to(jnp.array([[0.0, 0.75, 0.0],\n",
    "                                [0.0, 0.25, 1.0],\n",
    "                                [1.0, 0.0, 0.0]]\n",
    "            ), (2, 3, 3))\n",
    "\n",
    "B_2 = jnp.broadcast_to(jnp.array([[0.0, 0.25, 0.0],\n",
    "                                [0.0, 0.75, 0.0],\n",
    "                                [1.0, 0.0, 1.0]]\n",
    "            ), (2, 3, 3))\n",
    "\n",
    "B_uncontrollable = jnp.expand_dims(\n",
    "    jnp.broadcast_to(\n",
    "        jnp.array([[1.0, 0.0], [0.0, 1.0]]), (2, 2, 2)\n",
    "    ), \n",
    "    -1\n",
    ")\n",
    "\n",
    "B = [jnp.stack([B_1, B_2], axis=-1), B_uncontrollable]\n",
    "\n",
    "# create a policy-dependent sequence of B matrices\n",
    "\n",
    "policy_1 = jnp.array([ [0, 0],\n",
    "                        [1, 0],\n",
    "                        [1, 0] ]\n",
    "                    )\n",
    "\n",
    "policy_2 = jnp.array([ [1, 0],\n",
    "                        [1, 0],\n",
    "                        [1, 0] ]\n",
    "                    )\n",
    "\n",
    "policy_3 = jnp.array([ [1, 0],\n",
    "                        [0, 0],\n",
    "                        [1, 0] ]\n",
    "                    )\n",
    "\n",
    "all_policies = [policy_1, policy_2, policy_3]\n",
    "n_policies = len(all_policies)\n",
    "all_policies = list(jnp.stack(all_policies).transpose(2, 0, 1)) # `n_factors` lists, each with matrix of shape `(n_policies, n_time_steps)`\n",
    "\n",
    "# for the single modality, a sequence over time of observations (one hot vectors)\n",
    "obs = [jnp.broadcast_to(jnp.array([[1., 0., 0.], # observation 0 is ambiguous with respect to hidden state_1 and hidden_state 2\n",
    "                                    [0., 1., 0.],  # observation 1 yields certain inference over hidden_state_1 = 2\n",
    "                                    [0., 0., 1.], # observation 2 is ambiguous with respect to hidden state_1 and hidden_state 2\n",
    "                                    [1., 0., 0.]])[:, None], (4, 2, 3) )] # observation 0 is ambiguous with respect to hidden state_1 and hidden_state 2\n",
    "\n",
    "C = [jnp.ones((2,3))] # flat preferences\n",
    "D = [jnp.ones((2, 3)) / 3., jnp.ones((2, 2)) / 2.] # flat prior\n",
    "E = jnp.ones((2,n_policies))/n_policies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the `Agent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pA = None\n",
    "pB = None\n",
    "\n",
    "agents = Agent(\n",
    "        A=A,\n",
    "        B=B,\n",
    "        C=C,\n",
    "        D=D,\n",
    "        E=E,\n",
    "        pA=None,\n",
    "        pB=None,\n",
    "        policy_len=3,\n",
    "        control_fac_idx=None,\n",
    "        policies=None,\n",
    "        gamma=16.0,\n",
    "        alpha=16.0,\n",
    "        use_utility=True,\n",
    "        action_selection=\"deterministic\",\n",
    "        sampling_mode=\"full\",\n",
    "        inference_algo=\"ovf\",\n",
    "        num_iter=16,\n",
    "        learn_A=False,\n",
    "        learn_B=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `obs` and `policies`, pass in the arguments `outcomes`, `past_actions`, `empirical_prior` and `qs_hist` to `agent.infer_states(...)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run first timestep of inference using `obs[0]`, no past actions, empirical prior set to actual prior, no qs_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_obs = jtu.tree_map(lambda x: x[0], obs)\n",
    "beliefs = agents.infer_states(first_obs, past_actions=None, empirical_prior=D, qs_hist=None, mask=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate the last beliefs, aka output of the previous step (will now be passed in as `qs_hist`), assume the agent took the first timestep of each policy and log it as the `past_action`, and get the `empirical_prior` for the next step using `agent.update_empirical_prior()`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymdp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
